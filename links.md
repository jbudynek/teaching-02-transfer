# Collection of links

## NLP Models

- 2013 - Word2Vec
  - Mikolov, Tomas, et al. "Efficient estimation of word representations in
vector space." arXiv preprint arXiv:1301.3781 (2013).
  - <https://arxiv.org/abs/1301.3781>
- 2017 - Attention, Transformers
  - Vaswani, Ashish, et al. "Attention is all you need." Advances in neural
information processing systems 30 (2017).
  - <https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html>
- Qiu, Xipeng, et al. "Pre-trained models for natural language processing:
A survey." Science China Technological Sciences 63.10 (2020): 1872-1897.
  - <https://arxiv.org/abs/2003.08271>
- Han, Xu, et al. "Pre-trained models: Past, present and future."
AI Open 2 (2021): 225-250.
  - <https://www.sciencedirect.com/science/article/pii/S2666651021000231>

## Large Language Models

- Kojima, Takeshi, et al. "Large language models are zero-shot reasoners."
Advances in neural information processing systems 35 (2022): 22199-22213.
  - <https://arxiv.org/abs/2205.11916>

- Bender, Emily M., et al. "On the dangers of stochastic parrots: Can language
models be too big?ðŸ¦œ." Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency. 2021.
  - <https://dl.acm.org/doi/10.1145/3442188.3445922>

- Brown, Tom, et al. "Language models are few-shot learners."
Advances in neural information processing systems 33 (2020): 1877-1901.
  - <https://arxiv.org/abs/2005.14165>

## Computer vision models

- 2012 - Alexnet - CNN
  - Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet
classification with deep convolutional neural networks." Communications of the
ACM 60.6 (2017): 84-90.
  - <https://dl.acm.org/doi/pdf/10.1145/3065386>
- 2020 - Vision Transformers
  - Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers
for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
  - <https://arxiv.org/abs/2010.11929>
- Pre-trained models in Pytorch
  - <https://pytorch.org/vision/stable/models.html>
- Pre-trained models in Keras
  - <https://keras.io/api/applications/>
- Pre-trained models in PaddlePaddle OCR
  - <https://github.com/PaddlePaddle/PaddleOCR>

## Time Series

- 2023 - TimeGPT
  - Garza, Azul, and Max Mergenthaler-Canseco. "TimeGPT-1."
arXiv preprint arXiv:2310.03589 (2023).
  - <https://arxiv.org/abs/2310.03589>
  - <https://docs.nixtla.io/>

## Tabular data

- Hollmann, Noah, et al. "TabPFN: A Transformer That Solves Small Tabular
Classification Problems in a Second." arXiv preprint arXiv:2207.01848 (2022).
  - <https://arxiv.org/abs/2207.01848>
  - <https://www.automl.org/tabpfn-a-transformer-that-solves-small-tabular-classification-problems-in-a-second/>

## Transfer Learning

- Pan, Sinno Jialin, and Qiang Yang. "A survey on transfer learning."
IEEE Transactions on knowledge and data engineering 22.10 (2009): 1345-1359.
  - <https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf>
- Zhuang, Fuzhen, et al. "A comprehensive survey on transfer learning."
Proceedings of the IEEE 109.1 (2020): 43-76.
  - <https://arxiv.org/abs/1911.02685>
- Tan, Chuanqi, et al. "A survey on deep transfer learning." International
conference on artificial neural networks. Springer, Cham, 2018.
  - <https://arxiv.org/abs/1808.01974>

## Cast defects dataset

- <https://www.kaggle.com/datasets/ravirajsinh45/real-life-industrial-dataset-of-casting-product>

## Tansfer learning in Keras tutorial
- <https://keras.io/guides/transfer_learning/>

## Misc

- Self supervised learning
  - <https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/>
